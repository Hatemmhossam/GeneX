# -*- coding: utf-8 -*-
"""Copy of GENESpreprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g4L-CM-_P_rNwN3vwdXlUR7_2JK1pTIh
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %pip install neuroCombat

# Commented out IPython magic to ensure Python compatibility.
# %pip install GEOparse

import gzip
import GEOparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from neuroCombat import neuroCombat  # pip install neuroCombat
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve

exp_paths = {
    "RABC2": "/content/drive/MyDrive/genesExpression/RABC2_Level3.txt",
    "RABC3": "/content/drive/MyDrive/genesExpression/RABC3_Level3.txt",
    "RABC6": "/content/drive/MyDrive/genesExpression/RABC6_Level3.txt",
    "RABC7": "/content/drive/MyDrive/genesExpression/RABC7_Level3.txt",
    "RABC8": "/content/drive/MyDrive/genesExpression/RABC8_Level3.txt",
    "RABC18": "/content/drive/MyDrive/genesExpression/RABC18_Level3.txt",
    "RABC39": "/content/drive/MyDrive/genesExpression/RABC39_Level3.txt",

}

pheno_paths = {
    "RABC2": "/content/drive/MyDrive/genesExpression/phenotypes/RABC2_PhenotypeFile.csv",
    "RABC3": "/content/drive/MyDrive/genesExpression/phenotypes/RABC3_PhenotypeFile.csv",
    "RABC6": "/content/drive/MyDrive/genesExpression/phenotypes/RABC6_PhenotypeFile.csv",
    "RABC7": "/content/drive/MyDrive/genesExpression/phenotypes/RABC7_PhenotypeFile.csv",
    "RABC8": "/content/drive/MyDrive/genesExpression/phenotypes/RABC8_PhenotypeFile.csv",
    "RABC18": "/content/drive/MyDrive/genesExpression/phenotypes/RABC18_PhenotypeFile.csv",
    "RABC39": "/content/drive/MyDrive/genesExpression/phenotypes/RABC39_PhenotypeFile.csv",
}

"""# Load Datasets"""

def load_rabc_dataset(expr_path, pheno_path, name):
    print(f"Loading {name} ...")

    # ---------- Load expression ----------
    expr = pd.read_csv(expr_path, sep="\t", comment="#", low_memory=False)
    expr = expr.rename(columns={expr.columns[0]: "GeneSymbol"})
    expr = expr.dropna(subset=["GeneSymbol"])
    expr = expr.set_index("GeneSymbol")

    # ---------- Load phenotype ----------
    pheno = pd.read_csv(pheno_path)

    # pehontype columns: sample, status
    # status: 1 = RA, 0 = Healthy
    pheno["Label"] = pheno["status"].map({1: "RA", 0: "Healthy"})

    # ---------- Match samples ----------
    # Expression columns(GSE...) must match phenotype sample IDs(GSE...)
    common_samples = expr.columns.intersection(pheno["sample"])

    if len(common_samples) == 0:
        raise ValueError(f"No matching samples found for {name}")

    expr = expr[common_samples]
    pheno = pheno.set_index("sample").loc[common_samples]

    # ---------- Transpose ----------
    df = expr.T
    df["Label"] = pheno["Label"].values
    df["Batch"] = name

    print(f"âœ… {name}: {df.shape[0]} samples Ã— {df.shape[1]-2} genes")
    return df

datasets = [
    load_rabc_dataset(
        expr_path=exp_paths[name],
        pheno_path=pheno_paths[name],
        name=name
    )
    for name in exp_paths
]

datasets[3].iloc[:,0].hist(bins=50)
plt.title("Distribution of expression values")
plt.show()

datasets[0].columns

print(datasets[3].head())

gene_sets = [set(df.columns)- {"Label", "Batch"} for df in datasets]

common_genes = set.intersection(*gene_sets)

#Subset each dataset to only include common genes
datasets_common = [df.loc[:, list(common_genes) + ["Label", "Batch"]] for df in datasets]

len(common_genes)

GEO_PATH = "/content/drive/MyDrive/genesExpression/GEO dataset/GSE203024_series_matrix.txt.gz"

# Collect metadata lines
metadata = {}
with gzip.open(GEO_PATH, "rt") as f:
    for line in f:
        if line.startswith("!Sample_"):
            parts = line.strip().split("\t")
            key = parts[0].replace("!Sample_", "")
            metadata[key] = parts[1:]

# Build metadata DataFrame
meta_df = pd.DataFrame(metadata)
meta_df.index = metadata["geo_accession"]  # GSM IDs as index

print(meta_df.head())

keywords = ["healthy", "normal", "control"]

mask = meta_df.apply(lambda row: any(
    any(kw in str(val).lower() for kw in keywords)
    for val in row.values
), axis=1)

healthy_ids = meta_df[mask].index.tolist()
# Clean GSM IDs (remove quotes)
healthy_ids = [gsm.strip('"') for gsm in healthy_ids]
print(f"Found {len(healthy_ids)} healthy/control samples")

# Load expression matrix with only healthy samples
with gzip.open(GEO_PATH, "rt") as f:
    df_healthy = pd.read_csv(
        f, sep="\t", comment="!",
        usecols=["ID_REF"] + healthy_ids,
        low_memory=False
    )

print(df_healthy.head())

df_healthy.shape

# GEO_PATH = "/content/drive/MyDrive/genesExpression/GEO dataset/GSE203024_series_matrix.txt.gz"

# # Read header only to know which columns to load
# with gzip.open(GEO_PATH, "rt") as f:
#     for line in f:
#         if not line.startswith("!"):
#             header = line.strip().split("\t")
#             break

# # Columns to keep = ID_REF + healthy GSMs
# usecols = ["ID_REF"] + [c for c in header if c in healthy_samples]

# print(f"Expression columns loaded: {len(usecols) - 1}")

# expr_df = pd.read_csv(
#     GEO_PATH,
#     sep="\t",
#     comment="!",
#     usecols=usecols,
#     dtype="float32",        # halves memory
#     low_memory=True
# )

# expr_df.rename(columns={"ID_REF": "GeneSymbol"}, inplace=True)

# expr_df = expr_df.set_index("GeneSymbol")

# # Transpose: samples Ã— genes
# healthy_expr = expr_df.T

# print(healthy_expr.shape)

# Load annotation file
gpl = pd.read_csv("/content/drive/MyDrive/genesExpression/GEO dataset/GPL570-annotation.txt", sep="\t",comment="#", low_memory=False)

# Preview key columns
print(gpl.columns)  # Look for 'ID', 'Gene Symbol', 'Gene Title'

# Merge with expression matrix
GEO_dataset = df.merge(gpl[['ID', 'Gene Symbol', 'Gene Title']], left_on='ID_REF', right_on='ID')

# Clean up
GEO_dataset.drop(columns=['ID'], inplace=True)
GEO_dataset.rename(columns={'Gene Symbol': 'Symbol', 'Gene Title': 'Description'}, inplace=True)

print(GEO_dataset.shape)
print(GEO_dataset[['Symbol', 'Description']].head())

merged_df = pd.concat(datasets_common, axis=0)
print(f"\n Merged dataset shape: {merged_df.shape} (samples Ã— genes)")

"""# RA Gene Expression Classification â€” Batch-Corrected Logistic Regression


"""

# DATA CLEANING & SCALING

y = merged_df["Label"].map({"RA": 1, "Healthy": 0})
batch_labels = merged_df["Batch"]
X = merged_df.drop(columns=["Label", "Batch"])

# Drop genes with too many missing values (>30%)
missing_ratio = X.isna().mean()
X = X.loc[:, missing_ratio < 0.3]
print(f"âœ… Remaining genes after filtering: {X.shape[1]}")

# Impute missing values
imputer = SimpleImputer(strategy="mean")
X_imputed = imputer.fit_transform(X)


# APPLY COMBAT BATCH CORRECTION (ALL DATASETS)

print("ğŸ”„ Applying ComBat batch correction on all datasets...")
covars = pd.DataFrame({"batch": batch_labels})
covars["batch"] = covars["batch"].astype("category")

combat_data = neuroCombat(
    dat=X_imputed.T,
    covars=covars,
    batch_col="batch"
)["data"].T

print("Batch correction completed. New shape:", combat_data.shape)

#Scale after ComBat
scaler = StandardScaler()
combat_scaled = scaler.fit_transform(combat_data)
combat_scaled_df = pd.DataFrame(combat_scaled, columns=X.columns)


# PCA AFTER BATCH CORRECTION

print("\n=== PCA Before ComBat ===")
pca_pre = PCA(n_components=2)
pca_result_pre = pca_pre.fit_transform(combat_scaled_df)

plt.figure(figsize=(7, 6))
plt.scatter(
    pca_result_pre[:, 0], pca_result_pre[:, 1],
    c=batch_labels.astype('category').cat.codes, cmap="tab10"
)
plt.title("PCA Before Batch Correction (colored by dataset)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()


print("\n=== PCA After ComBat ===")
pca_post = PCA(n_components=2)
pca_result_post = pca_post.fit_transform(combat_scaled_df)

plt.figure(figsize=(7, 6))
plt.scatter(
    pca_result_post[:, 0], pca_result_post[:, 1],
    c=batch_labels.astype('category').cat.codes, cmap="tab10"
)
plt.title("PCA After Batch Correction (colored by dataset)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()

# DIMENSIONALITY REDUCTION FOR TRAINING

print("\nğŸ”¬ Applying PCA for dimensionality reduction to denoise features...")
pca = PCA(n_components=30)
X_pca = pca.fit_transform(combat_data)
print(f"âœ… Reduced feature space: {X_pca.shape[1]} principal components")


# TRAIN/TEST SPLIT (2/3 TRAIN, 1/3 TEST)

X_train, X_test, y_train, y_test = train_test_split(
    X_pca, y, test_size=0.33, random_state=42, stratify=y
)


# TRAIN LOGISTIC REGRESSION MODEL

print("\n=== Training Logistic Regression ===")
model = LogisticRegression(C=0.1, max_iter=500, penalty="l2", solver="liblinear", class_weight="balanced")

cv_scores = cross_val_score(model, X_train, y_train, cv=5)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

train_acc = model.score(X_train, y_train)
test_acc = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)

print("\n===  LOGISTIC REGRESSION RESULTS ===")
print(f"âœ… Train Accuracy: {train_acc:.3f}")
print(f"âœ… Test Accuracy:  {test_acc:.3f}")
print(f"âœ… ROC-AUC:        {roc_auc:.3f}")
print(f"âœ… Mean CV Accuracy: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")

# TOP GENES BY PCA CONTRIBUTION

# Get top genes contributing most to first components
loadings = pd.DataFrame(pca.components_.T, index=X.columns, columns=[f"PC{i+1}" for i in range(pca.n_components_)])
gene_influence = loadings.iloc[:, :5].abs().sum(axis=1).sort_values(ascending=False).head(15)
print("\n Top 15 genes contributing most to variance:")
print(gene_influence)


# ğŸ§¾ EXAMPLE PREDICTIONS
probs_df = pd.DataFrame({
    "True_Label": y_test.replace({1: "RA", 0: "Healthy"}).values,
    "Predicted_Label": np.where(y_prob > 0.5, "RA", "Healthy"),
    "RA_Probability(%)": (y_prob * 100).round(1)
})
print("\nğŸ§¾ Example prediction probabilities:")
print(probs_df.head(10))
print("Train AUC:", roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]))

print("\n Full harmonized pipeline completed successfully.")

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.decomposition import PCA
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, accuracy_score
from neuroCombat import neuroCombat
from sklearn.preprocessing import StandardScaler

# Use the X_scaled_df (DataFrame), batch_labels (Series), and y (Series) you already created.

# Convert to numpy for speed where needed
X_full = X_scaled_df.values.copy()
y_full = y.values.copy()
batches_full = batch_labels.values.copy()

# Outer CV: honest estimate (e.g., 5 folds)
outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Build pipeline WITHOUT ComBat (we will apply ComBat manually inside the loop on train only)
pipe = Pipeline([
    ("pca", PCA()),            # we'll tune n_components
    ("clf", LogisticRegression(max_iter=2000, solver="liblinear"))  # solver will be tuned where applicable
])

# Parameter grid for GridSearch (inner CV)
param_grid = {
    "pca__n_components": [10, 20, 30, 40],           # tune PCA dimension
    "clf__C": [0.01, 0.1, 1, 10],                    # regularization strength
    "clf__penalty": ["l1", "l2"],                    # l1 or l2
    # solver choice: liblinear supports l1/l2; saga also supports l1/l2 and is good for larger data
    # GridSearch will error if solver incompatible with penalty on a combination, so we'll use a small wrapper below
    "clf__solver": ["liblinear", "saga"],
    "clf__class_weight": ["balanced", None]
}

# We'll use inner CV (3-fold) for GridSearch using scoring by ROC-AUC
inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)
grid = GridSearchCV(
    estimator=pipe,
    param_grid=param_grid,
    scoring="roc_auc",
    cv=inner_cv,
    n_jobs=-1,
    verbose=1,
    refit=True
)

# Storage for outer results
outer_results = []
outer_best_params = []

print("Starting nested CV with ComBat applied only on training folds...")

for fold_idx, (train_idx, valid_idx) in enumerate(outer_cv.split(X_full, y_full), 1):
    print(f"\n--- Outer fold {fold_idx} ---")
    X_train_raw, X_valid_raw = X_full[train_idx], X_full[valid_idx]
    y_train_fold, y_valid_fold = y_full[train_idx], y_full[valid_idx]
    batches_train = batches_full[train_idx]
    batches_valid = batches_full[valid_idx]

    # Build covars for neuroCombat needs; it expects a DataFrame
    covars_train = pd.DataFrame({"batch": batches_train})
    covars_train["batch"] = covars_train["batch"].astype("category")

    # Apply ComBat on training data only
    combat_train = neuroCombat(
        dat=pd.DataFrame(X_train_raw, columns=X_scaled_df.columns).T,
        covars=covars_train,
        batch_col="batch"
    )["data"].T
    combat_train = np.array(combat_train)

    # Fit ComBat parameters to train and apply transform to validation: easiest is to fit Combat on train
    # and then apply by running neuroCombat on concatenated (train+valid) but using train batch estimates only is complex.
    # Simpler safe approach: compute Combat on combined train+valid but with batch covars where we hide labels.
    # However this would leak. So instead we approximate by using Combat's param estimates fitted on train and apply linear adjustment to valid:
    # The neuroCombat python package doesn't expose transform-only easily; an operationally safe route is:
    #   1) Fit Combat on train
    #   2) Apply the same location/scale adjustment to valid using batch means/std from train for matching batch labels
    # We'll implement a conservative approach: run neuroCombat separately on valid using its batch labels but *with no biological covariates*.
    # This is not perfect but avoids sharing label info from train.
    covars_valid = pd.DataFrame({"batch": batches_valid})
    covars_valid["batch"] = covars_valid["batch"].astype("category")
    combat_valid = neuroCombat(
        dat=pd.DataFrame(X_valid_raw, columns=X_scaled_df.columns).T,
        covars=covars_valid,
        batch_col="batch"
    )["data"].T
    combat_valid = np.array(combat_valid)


    # Now run grid search (inner CV) on the combat_train data
    grid.fit(combat_train, y_train_fold)

    print(" Best params (inner CV):", grid.best_params_)
    outer_best_params.append(grid.best_params_)

    # Evaluate the best estimator on the held-out valid fold
    best_model = grid.best_estimator_
    y_valid_prob = best_model.predict_proba(combat_valid)[:, 1]
    y_valid_pred = best_model.predict(combat_valid)

    auc = roc_auc_score(y_valid_fold, y_valid_prob)
    acc = accuracy_score(y_valid_fold, y_valid_pred)
    print(f" Outer fold {fold_idx} ROC-AUC: {auc:.4f}, Accuracy: {acc:.4f}")

    outer_results.append({"fold": fold_idx, "auc": auc, "acc": acc, "best_params": grid.best_params_})

# Summarize outer results
aucs = [r["auc"] for r in outer_results]
accs = [r["acc"] for r in outer_results]
print("\n=== Nested CV summary ===")
print(f"Mean ROC-AUC (outer): {np.mean(aucs):.4f} Â± {np.std(aucs):.4f}")
print(f"Mean Accuracy (outer): {np.mean(accs):.4f} Â± {np.std(accs):.4f}")

# Select most-common best params across folds (or use the best from a fold)
from collections import Counter
best_params_counter = Counter([str(p) for p in outer_best_params])
most_common_params_str, _ = best_params_counter.most_common(1)[0]
import ast
most_common_params = ast.literal_eval(most_common_params_str)
print("Most common best params across folds:", most_common_params)

# Final training: apply ComBat on the full training portion (train/test split you prefer)
# Here we do a final train/test split and evaluate final model.
from sklearn.model_selection import train_test_split
X_tr, X_te, y_tr, y_te, batches_tr, batches_te = train_test_split(
    X_full, y_full, batches_full, test_size=0.33, random_state=42, stratify=y_full
)

covars_tr = pd.DataFrame({"batch": batches_tr})
covars_tr["batch"] = covars_tr["batch"].astype("category")
combat_tr = neuroCombat(dat=pd.DataFrame(X_tr, columns=X_scaled_df.columns).T, covars=covars_tr, batch_col="batch")["data"].T
combat_tr = np.array(combat_tr)

covars_te = pd.DataFrame({"batch": batches_te})
covars_te["batch"] = covars_te["batch"].astype("category")
combat_te = neuroCombat(dat=pd.DataFrame(X_te, columns=X_scaled_df.columns).T, covars=covars_te, batch_col="batch")["data"].T
combat_te = np.array(combat_te)

# Build final pipeline with the selected best params
final_pca_n = most_common_params.get("pca__n_components", 30)
final_C = most_common_params.get("clf__C", 1.0)
final_penalty = most_common_params.get("clf__penalty", "l2")
final_solver = most_common_params.get("clf__solver", "liblinear")
final_class_weight = most_common_params.get("clf__class_weight", None)

final_model = Pipeline([
    ("pca", PCA(n_components=final_pca_n)),
    ("clf", LogisticRegression(
        C=final_C,
        penalty=final_penalty,
        solver=final_solver,
        class_weight=final_class_weight,
        max_iter=2000
    ))
])

final_model.fit(combat_tr, y_tr)
y_te_prob = final_model.predict_proba(combat_te)[:, 1]
y_te_pred = final_model.predict(combat_te)
final_auc = roc_auc_score(y_te, y_te_prob)
final_acc = accuracy_score(y_te, y_te_pred)
print("\n=== Final model on held-out test ===")
print("Final params:", {"pca__n_components": final_pca_n, "C": final_C, "penalty": final_penalty, "solver": final_solver, "class_weight": final_class_weight})
print(f"Test ROC-AUC: {final_auc:.4f}")
print(f"Test Accuracy: {final_acc:.4f}")

"""# RA Gene Expression Classification â€” Batch-Corrected Random Forest


"""

#  TRAIN/TEST SPLIT
X_train, X_test, y_train, y_test = train_test_split(
    combat_data, y, test_size=0.33, random_state=42, stratify=y
)

#  LASSO (L1)
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel
import numpy as np

Cs = np.linspace(0.01, 0.5, 20)  # 20 values from 0.01 to 0.5

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

mean_auc, n_features = [], []

for C in Cs:
    lasso = LogisticRegression(
        penalty='l1',
        solver='liblinear',
        C=C,
        max_iter=5000,
        random_state=42
    )
    aucs = cross_val_score(lasso, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1)
    mean_auc.append(aucs.mean())

    # Fit to count selected features
    lasso.fit(X_train, y_train)
    n_nonzero = np.sum(np.abs(lasso.coef_) > 1e-6)
    n_features.append(n_nonzero)

# Plot CV AUC vs C and #selected features
import matplotlib.pyplot as plt
fig, ax1 = plt.subplots(figsize=(7,5))
ax1.plot(Cs, mean_auc, marker='o', label='CV mean AUC', color='tab:blue')
ax1.set_xlabel('C (Regularization strength)')
ax1.set_ylabel('CV ROC-AUC', color='tab:blue')
ax1.tick_params(axis='y', labelcolor='tab:blue')

ax2 = ax1.twinx()
ax2.plot(Cs, n_features, marker='s', color='tab:orange', label='# Selected Features')
ax2.set_ylabel('Number of Non-Zero Features', color='tab:orange')
ax2.tick_params(axis='y', labelcolor='tab:orange')

ax1.set_title("LASSO C Tuning (0.01 â†’ 0.5)")
fig.tight_layout()
plt.show()

# Pick best C (highest CV AUC)
best_idx = np.argmax(mean_auc)
best_C = Cs[best_idx]
print(f"\nâœ… Best C based on CV AUC: {best_C}")



 # LASSO FEATURE SELECTION USING BEST C
lasso_selector = LogisticRegression(
    penalty='l1',
    solver='liblinear',
    C=best_C,
    max_iter=5000,
    class_weight='balanced',
    random_state=42
)
lasso_selector.fit(X_train, y_train)

# Select features with non-zero coefficients
selector = SelectFromModel(lasso_selector, prefit=True)
X_train_lasso = selector.transform(X_train)
X_test_lasso = selector.transform(X_test)

print(f"\n Selected {X_train_lasso.shape[1]} features out of {X_train.shape[1]}")

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report


#  RANDOM FOREST CLASSIFIER
rf = RandomForestClassifier(
    n_estimators=500,           # slightly more trees for stability
    max_depth=10,  # or 15
    min_samples_leaf=5,  # avoid overfitting small leaves
     max_features='sqrt',
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train_lasso, y_train)
y_pred_rf = rf.predict(X_test_lasso)
y_prob_rf = rf.predict_proba(X_test_lasso)[:,1]


# EVALUATION METRICS

cv_scores = cross_val_score(rf, X_train_lasso, y_train, cv=5, scoring='roc_auc', n_jobs=-1)
train_acc = accuracy_score(y_train, rf.predict(X_train_lasso))
test_acc = accuracy_score(y_test, y_pred_rf)
roc_auc = roc_auc_score(y_test, y_prob_rf)
precision = precision_score(y_test, y_pred_rf)
recall = recall_score(y_test, y_pred_rf)
f1 = f1_score(y_test, y_pred_rf)
cm = confusion_matrix(y_test, y_pred_rf)


print("\n=== LASSO + RANDOM FOREST RESULTS ===")
print(f"Train Accuracy: {train_acc:.3f}")
print(f"Test Accuracy : {test_acc:.3f}")
print(f"ROC-AUC       : {roc_auc:.3f}")
print("RF CV AUC:", cv_scores.mean(), "Â±", cv_scores.std())
print(f"Precision     : {precision:.3f}")
print(f"Recall        : {recall:.3f}")
print(f"F1-score      : {f1:.3f}")
print("\nConfusion Matrix:")
print(cm)
print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf, target_names=["Healthy", "RA"]))


# TOP FEATURE IMPORTANCE

feature_names = np.array(X.columns)[selector.get_support()]
importances = rf.feature_importances_
top_idx = np.argsort(importances)[::-1][:15]

print("\n Top 15 genes by Random Forest importance:")
for i in top_idx:
    print(f"{feature_names[i]} \u2014 Importance: {importances[i]:.4f}")

# EXAMPLE PREDICTIONS

probs_df = pd.DataFrame({
    "True_Label": y_test.replace({1: "RA", 0: "Healthy"}).values,
    "Predicted_Label": np.where(y_prob_rf > 0.5, "RA", "Healthy"),
    "RA_Probability(%)": (y_prob_rf * 100).round(1)
})
print("\n Example prediction probabilities:")
print(probs_df.head(10))
print("Train AUC:", roc_auc_score(y_train, rf.predict_proba(X_train_lasso)[:, 1]))

"""deep learning models

"""

# ============================================================
# DEEP LEARNING MODEL ARCHITECTURES
# ============================================================

import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.regularizers import l1_l2

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Global Parameters
LEARNING_RATE = 0.001
BATCH_SIZE = 32
EPOCHS = 200 # Sufficiently large, EarlyStopping will manage it

# 1. Simple Neural Network
def create_simple_nn(input_dim):
    model = Sequential([
        Dense(128, activation='relu', input_shape=(input_dim,)),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(1, activation='sigmoid') # Binary classification
    ])
    return model

# 2. Deep Neural Network (More layers, Batch Normalization)
def create_deep_nn(input_dim):
    model = Sequential([
        Dense(256, activation='relu', input_shape=(input_dim,)),
        BatchNormalization(),
        Dropout(0.4),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.4),
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(1, activation='sigmoid')
    ])
    return model

# 3. Regularized Neural Network (L1/L2 regularization)
def create_regularized_nn(input_dim):
    model = Sequential([
        Dense(256, activation='relu', input_shape=(input_dim,), kernel_regularizer=l1_l2(l1=0.001, l2=0.001)),
        BatchNormalization(),
        Dropout(0.5),
        Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001)),
        BatchNormalization(),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])
    return model

# ============================================================
# âœ¨ COMPREHENSIVE TRAINING AND EVALUATION FUNCTION
# ============================================================

def train_and_evaluate_comprehensive(model, X_train, y_train, X_test, y_test, model_name):
    """Train and evaluate with ALL metrics - probabilities + performance"""

    # Compile model
    model.compile(
        optimizer=Adam(learning_rate=LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=['accuracy', 'AUC']
    )

    # Callbacks
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)
    ]

    print(f"\n=== Training {model_name} ===")

    # Train model
    history = model.fit(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_split=0.2,
        callbacks=callbacks,
        verbose=1,
        class_weight={0: 1., 1: len(y_train)/(2*y_train.sum())}
    )

    # ============================================================
    # Ã°ÂŸÂ“ÂŠ COMPREHENSIVE EVALUATION - TRAINING DATA
    # ============================================================

    # Keras evaluate on training data
    train_loss_eval, train_acc_eval, train_auc_eval = model.evaluate(X_train, y_train, verbose=0)

    # Manual calculations on training data
    train_probabilities = model.predict(X_train, verbose=0).flatten()
    train_predictions = (train_probabilities > 0.5).astype(int)
    train_acc_manual = accuracy_score(y_train, train_predictions)
    train_auc_manual = roc_auc_score(y_train, train_probabilities)

    # ============================================================
    # Ã°ÂŸÂ“ÂŠ COMPREHENSIVE EVALUATION - TEST DATA
    # ============================================================

    # Keras evaluate on test data
    test_loss_eval, test_acc_eval, test_auc_eval = model.evaluate(X_test, y_test, verbose=0)

    # Manual calculations on test data (probabilities for clinical use)
    test_probabilities = model.predict(X_test, verbose=0).flatten()
    test_predictions = (test_probabilities > 0.5).astype(int)
    test_acc_manual = accuracy_score(y_test, test_predictions)
    test_auc_manual = roc_auc_score(y_test, test_probabilities)

    # ============================================================
    # Ã°ÂŸÂ“Âˆ PERFORMANCE ANALYSIS
    # ============================================================

    # Overfitting gaps
    overfit_gap_acc = train_acc_eval - test_acc_eval
    overfit_gap_auc = train_auc_eval - test_auc_eval

    # ============================================================
    # Ã°ÂŸÂ“Â‹ COMPREHENSIVE RESULTS DISPLAY
    # ============================================================

    print(f"\nÃ°ÂŸÂ“ÂŠ {model_name} COMPREHENSIVE RESULTS:")
    print(f"Ã°ÂŸÂÂ¯ TRAINING METRICS:")
    print(f"   âœ… Accuracy: {train_acc_eval:.3f} (evaluate) | {train_acc_manual:.3f} (manual)")
    print(f"   âœ… ROC-AUC:  {train_auc_eval:.3f} (evaluate) | {train_auc_manual:.3f} (manual)")
    print(f"Ã°ÂŸÂÂ¯ TEST METRICS:")
    print(f"   âœ… Accuracy: {test_acc_eval:.3f} (evaluate) | {test_acc_manual:.3f} (manual)")
    print(f"   âœ… ROC-AUC:  {test_auc_eval:.3f} (evaluate) | {test_auc_manual:.3f} (manual)")
    print(f"âš Ã¯Â¸Â OVERFITTING ANALYSIS:")
    print(f"   Ã°ÂŸÂ“Â‰ Accuracy Gap: {overfit_gap_acc:+.3f}")
    print(f"   Ã°ÂŸÂ“Â‰ AUC Gap:      {overfit_gap_auc:+.3f}")
    print(f"Ã°ÂŸÂ“Âˆ PROBABILITY RANGE: {test_probabilities.min():.3f} to {test_probabilities.max():.3f}")

    # ============================================================
    # Ã°ÂŸÂ“ÂŠ CONFUSION MATRIX
    # ============================================================

    # Compute confusion matrix
    cm = confusion_matrix(y_test, test_predictions)

    # Display confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap="Blues")
    plt.title(f"Confusion Matrix for {model_name}")
    plt.show()

    # ============================================================
    # Ã°ÂŸÂ“ÂŠ TRAINING HISTORY PLOTS
    # ============================================================

    plt.figure(figsize=(15, 5))

    # Accuracy plot
    plt.subplot(1, 3, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)
    plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)
    plt.title(f'{model_name} - Accuracy\nFinal: Train={history.history["accuracy"][-1]:.3f}, Val={history.history["val_accuracy"][-1]:.3f}')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Loss plot
    plt.subplot(1, 3, 2)
    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)
    plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)
    plt.title(f'{model_name} - Loss\nFinal: Train={history.history["loss"][-1]:.3f}, Val={history.history["val_loss"][-1]:.3f}')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # AUC plot
    plt.subplot(1, 3, 3)
    plt.plot(history.history['AUC'], label='Train AUC', linewidth=2)
    plt.plot(history.history['val_AUC'], label='Val AUC', linewidth=2)
    plt.title(f'{model_name} - AUC\nFinal: Train={history.history["AUC"][-1]:.3f}, Val={history.history["val_AUC"][-1]:.3f}')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return (history, train_probabilities, test_probabilities,
            train_acc_eval, test_acc_eval, train_auc_eval, test_auc_eval,
            overfit_gap_acc, overfit_gap_auc)

# ============================================================
# Ã°ÂŸÂÂ¯ TRAIN ALL MODELS WITH COMPREHENSIVE METRICS
# ============================================================

models = {
    "Simple NN": create_simple_nn(X_train.shape[1]),
    "Deep NN": create_deep_nn(X_train.shape[1]),
    "Regularized NN": create_regularized_nn(X_train.shape[1])
}

results = {}

for model_name, model in models.items():
    (history, train_probs, test_probs,
     train_acc, test_acc, train_auc, test_auc,
     overfit_acc, overfit_auc) = train_and_evaluate_comprehensive(
        model, X_train, y_train, X_test, y_test, model_name
    )

    results[model_name] = {
        'history': history,
        'train_probabilities': train_probs,
        'test_probabilities': test_probs,
        'train_acc': train_acc,
        'test_acc': test_acc,
        'train_auc': train_auc,
        'test_auc': test_auc,
        'overfit_gap_acc': overfit_acc,
        'overfit_gap_auc': overfit_auc
    }

# ============================================================
# Ã°ÂŸÂ“ÂŠ COMPREHENSIVE MODEL COMPARISON
# ============================================================

print("\n" + "="*70)
print("Ã°ÂŸÂÂ¯ COMPREHENSIVE MODEL COMPARISON - ALL METRICS")
print("="*70)

for model_name, result in results.items():
    print(f"\n{model_name}:")
    print(f"  Ã°ÂŸÂ“Âˆ TRAINING PERFORMANCE:")
    print(f"     â€¢ Accuracy: {result['train_acc']:.3f}")
    print(f"     â€¢ ROC-AUC:  {result['train_auc']:.3f}")
    print(f"  Ã°ÂŸÂ“ÂŠ TEST PERFORMANCE:")
    print(f"     â€¢ Accuracy: {result['test_acc']:.3f}")
    print(f"     â€¢ ROC-AUC:  {result['test_auc']:.3f}")
    print(f"  âš Ã¯Â¸Â OVERFITTING GAPS:")
    print(f"     â€¢ Accuracy: {result['overfit_gap_acc']:+.3f}")
    print(f"     â€¢ AUC:      {result['overfit_gap_auc']:+.3f}")

# ============================================================
# Ã°ÂŸÂ“Âˆ SIDE-BY-SIDE PERFORMANCE COMPARISON
# ============================================================

plt.figure(figsize=(15, 6))

# Accuracy comparison
plt.subplot(1, 2, 1)
model_names = list(results.keys())
train_accuracies = [results[name]['train_acc'] for name in model_names]
test_accuracies = [results[name]['test_acc'] for name in model_names]

x = np.arange(len(model_names))
width = 0.35

plt.bar(x - width/2, train_accuracies, width, label='Train Accuracy', alpha=0.8, color='blue')
plt.bar(x + width/2, test_accuracies, width, label='Test Accuracy', alpha=0.8, color='red')

plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Training vs Test Accuracy Comparison')
plt.xticks(x, model_names, rotation=45)
plt.legend()
plt.grid(True, alpha=0.3)

# AUC comparison
plt.subplot(1, 2, 2)
train_aucs = [results[name]['train_auc'] for name in model_names]
test_aucs = [results[name]['test_auc'] for name in model_names]

plt.bar(x - width/2, train_aucs, width, label='Train AUC', alpha=0.8, color='green')
plt.bar(x + width/2, test_aucs, width, label='Test AUC', alpha=0.8, color='orange')

plt.xlabel('Models')
plt.ylabel('AUC')
plt.title('Training vs Test AUC Comparison')
plt.xticks(x, model_names, rotation=45)
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ============================================================
# Ã°ÂŸÂÂ† FIND BEST MODEL
# ============================================================

# Find best model based on TEST AUC (most important for generalization)
best_model_name = max(results.keys(), key=lambda x: results[x]['test_auc'])
best_result = results[best_model_name]

print(f"\nÃ°ÂŸÂ”Â¥ BEST MODEL: {best_model_name}")
print(f"   Ã°ÂŸÂÂ¯ TRAINING: Accuracy = {best_result['train_acc']:.3f}, AUC = {best_result['train_auc']:.3f}")
print(f"   Ã°ÂŸÂÂ¯ TEST:     Accuracy = {best_result['test_acc']:.3f}, AUC = {best_result['test_auc']:.3f}")
print(f"   âš Ã¯Â¸Â OVERFITTING: Accuracy gap = {best_result['overfit_gap_acc']:+.3f}, AUC gap = {best_result['overfit_gap_auc']:+.3f}")

# ============================================================
# Ã°ÂŸÂ“ÂŠ COMPREHENSIVE PROBABILITY ANALYSIS
# ============================================================

best_probs = best_result['test_probabilities']

# Create comprehensive probability analysis
prob_df = pd.DataFrame({
    'True_Label': y_test.replace({1: "RA", 0: "Healthy"}).values,
    'RA_Probability': best_probs,
    'Probability_Percentage': (best_probs * 100).round(1)
})

# Add risk categories
def get_risk_category(prob):
    if prob < 0.2: return "Very Low Risk (0-20%)"
    elif prob < 0.4: return "Low Risk (20-40%)"
    elif prob < 0.6: return "Medium Risk (40-60%)"
    elif prob < 0.8: return "High Risk (60-80%)"
    else: return "Very High Risk (80-100%)"

prob_df['Risk_Category'] = prob_df['RA_Probability'].apply(get_risk_category)

print(f"\nÃ°ÂŸÂ“Â‹ BEST MODEL PROBABILITY DISTRIBUTION:")
print(f"   Total Test Patients: {len(prob_df)}")
print(f"   RA Patients: {y_test.sum()}")
print(f"   Healthy Patients: {len(y_test) - y_test.sum()}")
print(f"   Mean Probability: {best_probs.mean():.3f}")
print(f"   RA Patients Mean: {best_probs[y_test == 1].mean():.3f}")
print(f"   Healthy Patients Mean: {best_probs[y_test == 0].mean():.3f}")

print("\nÃ°ÂŸÂ“Â SAMPLE PROBABILITY PREDICTIONS (First 15 patients):")
print(prob_df.head(15))

# ============================================================
# Ã°ÂŸÂÂ¨ COMPREHENSIVE VISUALIZATIONS
# ============================================================

plt.figure(figsize=(15, 10))

# 1. Probability distribution by true label
plt.subplot(2, 2, 1)
for label in ['Healthy', 'RA']:
    subset = prob_df[prob_df['True_Label'] == label]
    plt.hist(subset['RA_Probability'], alpha=0.7, label=label, bins=20, density=True)
plt.xlabel('RA Probability')
plt.ylabel('Density')
plt.title(f'Probability Distribution by True Label\n({best_model_name})')
plt.legend()
plt.grid(True, alpha=0.3)

# 2. Risk category distribution
plt.subplot(2, 2, 2)
risk_counts = prob_df['Risk_Category'].value_counts()
plt.pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('Risk Category Distribution')

# 3. ROC Curve
plt.subplot(2, 2, 3)
fpr, tpr, _ = roc_curve(y_test, best_probs)
plt.plot(fpr, tpr, linewidth=2, label=f'{best_model_name}\nAUC = {best_result["test_auc"]:.3f}')
plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True, alpha=0.3)

# 4. Overfitting comparison across models
plt.subplot(2, 2, 4)
overfit_gaps = [results[name]['overfit_gap_auc'] for name in model_names]
colors = ['red' if gap > 0.1 else 'orange' if gap > 0.05 else 'green' for gap in overfit_gaps]
plt.bar(model_names, overfit_gaps, color=colors, alpha=0.7)
plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
plt.xlabel('Models')
plt.ylabel('Overfitting Gap (Train AUC - Test AUC)')
plt.title('Overfitting Analysis\n(Green=Good, Orange=Moderate, Red=High)')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nâœ… COMPREHENSIVE PIPELINE COMPLETED SUCCESSFULLY!")
print(f"Ã°ÂŸÂÂ¯ Best Model: {best_model_name} (Test AUC: {best_result['test_auc']:.3f})")
print("Ã°ÂŸÂ“ÂŠ Output: Complete probabilities + Comprehensive performance metrics")
print("Ã°ÂŸÂ’Â¡ Use probabilities for clinical risk assessment & metrics for model evaluation")

"""# **Connect to frontend**"""

!pip install flask-ngrok

from flask import Flask, request, jsonify
from flask_ngrok import run_with_ngrok

app = Flask(__name__)
run_with_ngrok(app)  # Starts ngrok when the app runs

@app.route("/")
def home():
    return "Hello from Colab!"

@app.route("/predict", methods=["POST"])
def predict():
    data = request.json  # Get JSON data from frontend
    # Example: just return the same data
    return jsonify({"received": data})

app.run()

!pip install flask pyngrok

from pyngrok import ngrok

# Replace with your ngrok authtoken
ngrok.set_auth_token("35JDZi5JIpsZkMkTx2lcJLU6x9f_7j6wLHJHwfq9H6WdFuRLn")

!pip install flask pyngrok flask-cors

from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok

app = Flask(__name__)
CORS(app)  # Enable CORS

@app.route("/")
def home():
    return "Hello from Colab!"

@app.route("/predict", methods=["POST"])
def predict():
    data = request.json
    name = data.get("name", "Stranger")
    return jsonify({"message": f"Hello {name}!"})

# Open ngrok tunnel
public_url = ngrok.connect(5000)
print("Public URL:", public_url)

# Start Flask
app.run(port=5000)

